# 第三章 爬虫和蜘蛛

> 作者：Gilberto Najera-Gutierrez

> 译者：[飞龙](https://github.com/)

> 协议：[CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/)

## 简介

渗透测试可以通过多种途径完成，例如黑盒、灰盒和白盒。黑盒测试在测试者没有任何应用的前置信息条件下执行，除了服务器的 URL。白盒测试在测试者拥有目标的全部信息的条件下执行，例如它的构造、软件版本、测试用户、开发信息，以及其它。灰盒测试是黑盒和白盒的混合。

对于黑盒和灰盒测试，侦查阶段对测试者非常必然，以便发现白盒测试中通常由应用所有者提供的信息。

我们打算采取黑盒测试方式，因为它涉及到外部攻击者用于获取足够信息的所有步骤，以便入侵应用或服务器的特定功能。

作为每个 Web 渗透测试中侦查阶段的一部分，我们需要浏览器每个包含在网页中的链接，并跟踪它展示的每个文件。有一些工具能够帮助我们自动和以及加速完成这个任务，它们叫做 Web 爬虫或蜘蛛。这些工具通过跟随所有到外部文件的链接和引用，有的时候会填充表单并将它们发送到服务器，保存所有请求和响应来浏览网页，从而提供给我们离线分析它们的机会。

这一章中，我们会涉及到一些包含在 Kali 中的爬虫的使用，也会查看我们感兴趣的文件和目录，来寻找常见的网页。

## 3.1 使用 Wget 为离线分析下载网页

Wget 是 GNU 项目的一部分，也包含在主流 linux 发行版中，包括 Kali。它能够递归为离线浏览下载网页，包括链接转换和下载非 HTML 文件。

这个秘籍中，我们会使用 Wget 来下载和 vulnerable_vm 中的应用相关的页面。

### 准备

这一章的所有秘籍都需要运行 vulnerable_vm。在这本书的特定场景中，它的 IP 地址为 192.168.56.102。

### 操作步骤

1.  让我们做第一次尝试，通过仅仅以一个参数调用 Wget 来下载页面。

    ```
    wget http://192.168.56.102/bodgeit/
    ```
    
    ![](img/3-1-1.jpg)
    
    我们可以看到，它仅仅下载了`index.html`文件到当前目录，这是应用的首页。
    
2.  我们需要使用一些选项，告诉 Wget 将所有下载的文件保存到特定目录中，并且复制我们设为参数的 URL 中包含的所有文件。让我们首先创建目录来保存这些文件：

    ```
    mkdir bodgeit_offline
    ```
    
3.  现在，我们会递归下载应用中所有文件并保存到相应目录中。

    ```
    wget -r -P bodgeit_offline/ http://192.168.56.102/bodgeit/

    ```
    
    ![](img/3-1-2.jpg)
    
### 工作原理

像之前提到的那样，Wget 是个为下载 HTTP 内容创建的工具。通过`-r`参数，我们可以使其递归下载，这会按照它所下载的每个页面的所有连接，并同样下载它们。`-P`选项允许我们设置目录前缀，这是 Wget 会开始保存下载内容的目录。默认它设为当前目录。

### 更多

在我们使用 Wget 时，可以考虑一些其它的实用选项：

+   `-l`：在递归下载的时候，规定 Wget 的遍历深度可能很有必要。这个选项后面带有我们想要遍历的层级深度的数值，让我们规定这样的界限。

+   `-k`：在文件下载之后，Wget 修改所有链接，使其指向相应的本地文件，这会使站点能够在本地浏览。

+   `-p`：这个选项让 Wget 下载页面所需的所有图像，即使它们位于其它站点。

+   `-w`：这个选项让 Wget 在两次下载之间等待指定的描述。当服务器中存在防止自动浏览的机制时，这会非常有用。

## 3.2 使用 HTTrack 为离线分析下载页面

就像 HTTrack 的官网所说（`http://www.httrack.com`）：

> 它允许你从互联网下载 WWW 站点到本地目录中，递归构建所有目录、从服务器获得 HTML、图像，和其它文件到你的计算机中。

我们在这个秘籍中会使用 HTTrack 来下载应用站点的所有内容。

### 准备

HTTrack 没有默认在 Kali 中安装。所以我们需要安装它。

```
apt-get update 
apt-get install httrack
```

### 操作步骤

1.  我们的第一步是创建目录来储存下载的站点，输入：

    ```
    mkdir bodgeit_httrack 
    cd bodgeit_httrack
    ```
    
2.  使用 HTTrack 的最简单方式就是向命令中添加我们打算下载的 URL。

    ```
    httrack http://192.168.56.102/bodgeit/ 
    ```
    
    设置最后的`/`非常重要，如果遗漏了的话，HTTrack 会返回 404 错误，因为服务器根目录没有`bodgeit`文件。
    
    ![](img/3-2-1.jpg)
    
3.  现在，如果我们访问文件` file:///root/MyCookbook/test/bodgeit_httrack/index. html`（或者你在你的测试环境中选择的目录），我们会看到，我们可以离线浏览整个站点：

    ![](img/3-2-2.jpg)
    
### 工作原理

HTTrack 创建站点的完整静态副本，这意味着所有动态内容，例如用户输入的响应，都不会有效。在我们下载站点的文件夹中，我们可以看到下列文件和目录：

+   以服务器名称或地址命名的目录，包含所有下载的文件。

+   `cookies.txt`文件，包含用于下载站点的 cookie 信息。

+   `hts-cache`目录包含由爬虫检测到的文件列表，这是 httrack 所处理的文件列表。

+   `hts-log.txt `文件包含错误、警告和其它在爬取或下载站点期间的信息

+   ` index.html`文件重定向到副本的原始主页，它位于名称为服务器的目录中。

### 更多

HTTrack 也拥有一些扩展选项，允许我们自定义它的行为来更好符合我们的需求。下面是一些值得考虑的实用修改器：

+   `-rN`：将爬取的链接深度设置为 N。
+   `-%eN`：设置外部链接的深度界限。
+   `+[pattern]`：告诉 HTTrack 将匹配`[pattern]`的 URL 加入白名单，例如`+*google.com/*`。
+   `-[pattern]`：告诉 HTTrack 将匹配`[pattern]`的 URL 加入黑名单。
+   `-F [user-agent]`：允许我们定义用于下载站点的 UA（浏览器标识符）。
